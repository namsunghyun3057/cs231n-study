# 5강. 컨볼루션 신경망 (Convolutional Neural Networks, CNNs)  

**Outline**
- 이미지의 공간적 특성을 활용하고 효율적인 특징 학습을 가능하게 하는 CNN의 핵심 계층  


### 1. 배경 및 역사 (Motivation and History)

*   **FC 계층의 한계:** 완전 연결 계층(FC)은 3D 이미지를 1D 벡터로 **펼쳐(Stretch out)** 입력받아 **공간적 구조를 상실**  
    -> CNN은 이 구조를 **보존**하며 연산!  
*   **Hubel & Wiesel의 연구 (1950s):** 뉴런이 **지역적 연결성(Local Connectivity)**을 가지며 **계층적 조직(Hierarchical Organization)**(단순 세포 $\rightarrow$ 복합 세포)을 이룬다는 것을 발견
    -> CNN은 여기서 영감을 받음  
    *   **단순 세포(Simple Cells):** **방향성 에지(oriented edges)**에 반응
    *   **복합 세포(Complex Cells):** 에지 방향, 움직임, 코너, 끝점 등 **복잡한 특징**에 반응
*   **Neocognitron (1980):** Fukushima가 단순/복합 세포 아이디어를 모델링하여 CNN의 초기 개념을 제시
*   **LeCun의 적용 (1998):** Yann LeCun이 역전파를 사용하여 CNN을 문서 인식(우편번호 숫자 인식)에 적용  
    -> 단, 복잡한 데이터로는 확장되지 못했음  
*   **AlexNet (2012):** Alex Krizhevsky가 ImageNet 분류에서 강력한 결과를 달성하며 CNN의 광범위한 사용을 촉발  
    -> 이는 **대규모 데이터**와 **GPU 병렬 컴퓨팅 능력** 덕분!  

  
### 2. 컨볼루션 계층 (Convolutional Layer)의 작동

*   **필터 (Filter/Kernel):** 
    - 가중치(W)로 구성된 작은 3차원 행렬(예: $5 \times 5$ 공간 크기)  
    - 필터는 입력 볼륨의 **전체 깊이(full depth)**까지 확장됨
*   **연산:** 필터를 입력 볼륨 위로 공간적으로 **슬라이드(Slide)**하면서, 필터와 지역적 입력 패치 간의 **내적(Dot Product)**을 계산  
    -> 그 결과가 출력 **활성화 맵(Activation Map)**의 한 값이 됨  
*   **가중치 공유 (Parameter Sharing):** 슬라이드하는 모든 공간 위치에서 **동일한 필터(가중치 세트)**가 재사용됨  
    -> 이는 매개변수 수를 줄이고 **특징의 공간적 일반화**를 가능하게 함  
*   **지역적 연결성 (Local Connectivity):** 뉴런(출력 픽셀)은 전체 입력 대신 **지역적 영역(local region, 수용장(receptive field))**에만 연결됨
*   **출력 볼륨의 깊이:** 이 계층이 사용하는 **필터의 개수(K)**와 같음  

  
### 3. 컨볼루션 하이퍼파라미터 및 크기 계산

1.  **스트라이드 (Stride, S):** 필터가 슬라이드할 때 건너뛰는 픽셀 간격. 즉, 이동하는 정도  
    - 스트라이드가 클수록 출력이 **다운샘플링(Downsampled)**됨
2.  **제로 패딩 (Zero Padding, P):** 입력 이미지 가장자리에 0을 추가하여 경계를 확장
    *   **주요 목적:** 컨볼루션 후에도 **입력 볼륨의 공간적 크기를 유지(maintain the spatial size)**하여 네트워크가 깊어질 때 크기가 급격히 줄어드는 것을 방지!
3.  **출력 크기 공식 ($O$):** 입력 $N$, 필터 $F$, 패딩 $P$, 스트라이드 $S$일 때:
    $$O = \frac{N - F + 2P}{S} + 1$$
    *   이 공식은 풀링 계층에도 적용됨
4.  **1x1 컨볼루션:** 공간적으로는 1x1이지만, **전체 깊이 차원에 걸쳐 내적**을 수행하여 **깊이 차원(필터 개수)을 줄이는** 데 유용
5.  **일반적인 설정:** 
    - 필터 크기 $3\times 3, 5\times 5, 7\times 7$  
    - 필터 개수(깊이)는 보통 $32, 64, 128$ 등 **2의 거듭제곱**을 사용  

  
### 4. 풀링 계층 (Pooling Layer)

*   **목적:** 표현의 크기를 줄여 **관리가 용이하게(more manageable)** 만들고, 주어진 영역에 대한 **불변성(Invariance)**을 도입
*   **작동 방식:** **공간적으로** 풀링하며, **깊이(Depth) 차원에는 변화를 주지 않음**
*   **Max Pooling (최대값 풀링):** 가장 흔한 방법으로, 풀링 영역 내의 **최대값**을 취함  
    -> 이는 해당 영역 내 **어디에서든 특정 특징이 활성화되었는지**를 포착하는 것으로 해석  
*   **특징:** 일반적으로 $2 \times 2$ 필터와 스트라이드 2를 사용하여 **오버랩 없이** 다운샘플링  
    -> 풀링 계층에는 보통 제로 패딩을 **사용하지 않음** (최근에는 스트라이드 컨볼루션이 풀링을 대체하는 경향도 있음)  

  
### 5. 전체 CNN 아키텍처 (Full CNN Architecture)

*   **표준 구성:** CNN은 Conv 계층과 **ReLU**를 반복하고, 간헐적으로 **풀링 계층(Pool)**을 삽입하며, 최종적으로 **완전 연결(Fully Connected, FC) 계층**과 소프트맥스(Softmax)를 통해 클래스 점수를 산출
*   **계층적 특징 학습:**
    *   초기 Conv 계층은 에지(Edge) 같은 **저수준 특징**을 학습
    *   깊은 계층으로 갈수록 이러한 특징을 조합하여 코너, 블롭 등 **더 복잡한 고수준의 개념**을 학습
*   **FC 계층의 역할:** 마지막 Conv/Pool 출력 볼륨은 1차원 벡터로 **평탄화(Flatten)**되어 FC 계층에 연결  
    -> 이 FC 계층은 이전 계층의 **모든 복잡한 특징 정보**를 **집계(aggregating)**하여 최종 점수를 산출
*   **최근 추세:** 더 작은 필터($3 \times 3$ 등)를 사용하고 아키텍처를 **깊게** 가져가는 경향 있음