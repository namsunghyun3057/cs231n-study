# 3강. 손실 함수 및 최적화 (Loss Functions and Optimization)  

**Outline**
- 선형 분류기의 성능을 정량적으로 측정하는 방법인 **손실 함수(Loss Functions)**
- 최적의 가중치 행렬 W를 찾는 방법인 **최적화(Optimization)**  


## 1. Loss Function(손실 함수)
: 모델이 나타내는 확률 분포와 데이터의 실제 확률 분포 차이를 계산하는 함수

$$L = \frac{1}{N} \sum_{i=1}^{N} L_i (f(x_i, W), y_i)$$  

실제값과 예측값의 차이를 수치화하여 **오차**를 나타냄
-> $loss function의 값 \propto 오차$

- 손실 함수는 특정 가중치 W의 '나쁨' 정도를 정량화하는 방법
전체 데이터셋에 대한 최종 손실 $L$은 $N$개의 예제에 대한 개별 손실 $L_i$의 평균  


### 1-1. Multiclass SVM Loss (다중 클래스 SVM 손실)

$$\max(0, S_J - S_{y_i} + 1)$$

*   이진 SVM(Support Vector Machine)의 다중 클래스 버전
*   **원리:** 손실 $L_i$는 **정확한 클래스의 점수($S_{y_i}$)가 오답 클래스의 점수($S_J$)보다 설정된 안전 마진(safety margin, 보통 1로 설정)**만큼 높아야 함
*   위 형태를 **hinge loss(힌지 손실)**이라 함
*   **특징:**
    *   최소 손실: 0, 최대 손실: Infinity
    *   W를 무작위 작은 값으로 초기화하여 모든 점수(S)가 약 0일 때, 예상되는 초기 손실은 **클래스 수 - 1 ($C-1$)**  
        -> 이는 디버깅에 유용한 전략!
    *   SVM 손실은 일단 분류가 안전 마진을 충족하여 손실이 0이 되면, 해당 데이터 포인트에 대해 더 이상 점수를 개선하려 하지 않음  
        -> 그러니까, SVM 손실은 기준이 보다 느슨함!
    *   Squared Hinge Loss: 힌지 손실, 즉 $\max(0, \cdot)$ 항에 제곱을 추가하면 큰 실수를 더 크게 처벌하도록 가중치를 부여할 수 있음  
  
### Regularization (정규화)

*   **Overfitting(과적합):** 손실 함수만 사용하는 경우, 훈련 데이터를 완벽하게 맞추는 비현실적이고 복잡한 모델(점들을 모두 잇는 곡선 등)이 나오는 것  
    -> 그래서 필요한 것이 바로 정규화!
*   **Regularization(정규화)**는 손실 함수에 추가 항(패널티 항)을 더하여 모델이 **더 간단한 W**를 선택하도록 유도  
    -> 오컴의 면도날(Ockham's Razor) 원칙(: 가장 간단하게!)을 따름
*   **최종 손실:** $L = \text{데이터 손실} + \lambda \text{정규화 손실}(R(W))$. 이때, $\lambda$는 데이터 적합과 모델 복잡성 간의 균형을 조절하는 Hyperparameter(초매개변수)!
*   **주요 정규화 유형:**
    *   **L2 정규화 (Weight Decay, 가중치 감소):** $R(W) = \sum_{k}\sum_{l}W_{k,l}^2$ 가중치 벡터 W의 유클리드 노름(Euclidean norm)을 패널티로 부과  
        -> 이는 가중치의 영향력을 전체 입력 값에 걸쳐 **분산**시키려는 경향을 가짐  
        -> 베이즈 관점에서는 가우시안 사전 분포(Gaussian prior) 하의 MAP 추론으로 해석될 수 있음 ~~이게 뭐지~~
    *   **L1 정규화:** $R(W) = \sum_{k}\sum_{l}|W_{k,l}|$ 가중치 벡터 W의 L1 노름을 패널티로 부과하며, W에 **희소성(sparsity)**(많은 0)을 유도
    *   딥러닝에 특화된 정규화(Dropout, Batch Normalization 등)는 나중에 다룰 예정  

    
### 1-2. Softmax Loss / Multinomial Logistic Regression (소프트맥스 손실 / 다항 로지스틱 회귀)

$$\mathbf{-\log P(y_i|X; W)}$$

*   딥러닝에서 매우 일반적인 선택
*   **원리:** 선형 분류기에서 나오는 점수(scores)에 **추가적인 의미(확률)**를 부여
*   **Softmax function(소프트맥스 함수)**를 사용하여 점수(S)를 클래스에 대한 확률 분포로 변환(점수를 지수화한 후, 전체 지수 값의 합으로 정규화).
*   손실은 **정확한 클래스의 확률의 음의 로그(Negative log of the probability of the true class)**로 계산  
    -> 이는 계산된 확률 분포를 목표 확률 분포(정확한 클래스에 1, 나머지에 0)와 일치시키도록 권장  
*   **특징:**
    *   최소 손실: 0, 최대 손실: Infinity (단, 유한 정밀도(finite precision)에서는 0이나 무한대에 도달하기 어려움)
    *   W를 무작위 작은 값으로 초기화하여 모든 점수(S)가 약 0일 때, 예상되는 초기 손실은 **클래스 수의 로그 ($\log(C)$)**  
        -> 이는 디버깅에 유용한 전략!
    *   Softmax 손실은 **분류기가 지속적으로 개선되도록 유도**함. SVM과 달리, 이미 올바르게 분류되었더라도 정확한 클래스의 확률을 1로, 오답 클래스의 확률을 0으로 계속 개선함(점수를 무한대로, 마이너스 무한대로).  
        -> 그러니까, Softmax 손실은 기준이 보다 엄격함!  
  

## 2. Optimization (최적화)
: 손실 함수를 최소화하는 최적의 W를 찾는 과정  
-> 복잡한 함수에서는 분석적인 해법을 찾기 어렵기 때문에 반복적인 방법을 사용!

### 2-1. Gradient Descent (경사 하강법)

*   손실 함수가 형성하는 '계곡'을 걸어 다니며 가장 낮은 지점을 찾는 느낌
*   **Gradient(기울기, 여기에서는 경사):** 편도함수의 벡터, 함수의 **증가** 방향을 가리킴  
    -> 따라서 손실을 줄이려면 **음의 경사(-Gradient)** 방향으로 움직여야 함!
*   **경사 계산:**
    *   **수치적 경사(Numerical Gradient, Finite Differences):** 정의에 따라 작은 값 H를 사용하여 근사적으로 계산하지만, 매개변수가 수백만 개일 경우 **매우 느리다**고 함
    *   **분석적 경사(Analytic Gradient):** 미적분을 사용 계산 -> 실제 훈련에 사용 (단, 오류 발생 가능성 높음)
    *   **경사 확인(Gradient Checking):** 분석적 경사를 구현한 코드가 정확한지 디버깅하기 위해 수치적 경사를 유닛 테스트로 사용
*   **경사 하강법 알고리즘:**
    1. W를 무작위로 초기화.
    2. 손실과 경사를 계산.
    3. **가중치 업데이트:** $W \leftarrow W - \text{Step Size} \times \text{Gradient}$.
*   **학습률(Learning Rate / Step Size):** 경사 방향으로 얼마나 멀리 이동할지 결정하는 **가장 중요한 초매개변수** 중 하나  
    -> 점점 작아져야 정확한 계산이 가능함  

### 2-2. Stochastic Gradient Descent (확률적 경사 하강법, SGD)

*   훈련 데이터셋 $N$이 너무 클 경우(예: 백만 개 이상), 전체 데이터셋에 대한 손실 및 경사 계산이 매우 느려짐
*   SGD는 전체 데이터셋 대신 **미니 배치(mini batch)**라는 작은 데이터 샘플(일반적으로 32, 64, 128 등 2의 거듭제곱)을 사용하여 손실과 경사를 추정하고 W를 업데이트
*   SGD는 딥 뉴럴 네트워크 훈련에 사용되는 기본 알고리즘!  

  
## 3. Image Features (이미지 특징)

*   **CNN 이전 접근 방식:** 딥러닝 전에는 **원시 픽셀**을 선형 분류기에 직접 넣지 않음
    -> 접근 과정  
    1. **Feature Extractor(특징 추출기)**를 사용하여 이미지를 특징 표현으로 변환  
    2. 선형 분류기에 공급  
*   **목표:** 데이터셋을 선형 분류기로 분류할 수 있도록 **선형 분리 가능하게** 만들기
*   **특징 예시:**
    *   **Color Histogram(컬러 히스토그램):** 이미지에 존재하는 색상의 종류를 전역적으로 알려줌
    *   **Histogram of Oriented Gradients(방향성 기울기 히스토그램, HOG):** 이미지에서 국부적인 에지(Edge)의 방향을 측정
    *   **Bag of Words:** 자연어 처리에서 영감을 받아, 이미지 패치를 군집화하여 "시각적 단어(visual words)" 사전(코드북)을 만들고, 이미지에 이러한 단어가 얼마나 자주 발생하는지를 특징으로 사용
*   **딥러닝과의 연결:** 컨볼루션 신경망(Convolutional Neural Networks)은 고정된 특징 추출 블록 대신 **데이터 기반으로 특징을 직접 학습**하며, 전체 네트워크를 훈련 