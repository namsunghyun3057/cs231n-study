# 4강. 역전파와 신경망 (Backpropagation and Neural Networks)

**Outline**
- 복잡한 함수의 분석적 경사를 계산하는 **역전파(Backpropagation)**
- 역전파를 바탕으로 구축되는 **신경망(Neural Networks)**의 구조와 원리  


## 1. Backpropagation (역전파)
: 임의로 복잡한 함수에 대한 **분석적 경사(Analytic Gradient)**를 효율적으로 계산하기 위한 프레임워크  
-> 복잡한 수식을 계산 그래프로 시각화하여 미분을 쉽게!  

- 신경망 학습의 목표는 손실(Loss)을 최소화하는 가중치(W)를 찾는 것  
이를 위해 손실 함수 $L$에 대한 $W$의 기울기(Gradient)를 구해야 하는데, 수식이 복잡해지면 직접 미분하기 어려움  
-> 그래서 이때 사용하는 것이 역전파!  

  
### 1-1. Computational Graphs and Gradient Computation (계산 그래프와 경사 계산)

*   **계산 그래프:** 함수를 노드(간단한 계산 단계)로 분해해서 그래프 형태로 표현
*   **경사의 필요성:** 최적의 W를 찾으려면 손실 함수 $L$의 경사를 계산해야 함
    *   **수치적 경사 (Numerical Gradient):** **느리고 근사적**이지만 작성하기는 쉬움
    *   **분석적 경사 (Analytic Gradient):** **빠르고 정확**하지만 **실수하기 쉬움**
    *   **경사 확인 (Gradient Checking):** 분석적 경사를 유도한 후, 구현의 정확성을 확인하기 위해 수치적 경사를 **유닛 테스트(unit test)**로 사용
*   **역전파 원리:** 그래프의 끝(손실)에서 시작해서 **연쇄 법칙(Chain Rule)**을 재귀적으로 사용하여 거꾸로 이동하며 모든 변수의 경사를 계산
    *   **순전파 (Forward Pass):** 값을 계산하고 **중간 값들(intermediate values)을 저장(Cache)**  
        -> 이 값들이 역전파 시 지역적 경사 계산에 사용됨
    *   **상류 경사와 지역적 경사:** 각 노드는 Upstream에서 들어오는 **상류 경사(Upstream Gradient)**를 받아서, 노드의 **지역적 경사(Local Gradient)**와 곱해서 최종 경사를 산출

  
### 1-2. Gate Interpretation (게이트별 동작 및 직관)  

*   **Add Gate (덧셈 게이트):** **경사 분배기(Gradient Distributor)**, 상류 경사를 모든 입력 브랜치에 **그대로 분배**
*   **Multiply Gate (곱셈 게이트):** **경사 스위처/스케일러(Switcher/Scaler)**, 상류 경사에 **상대 브랜치(other branch)의 순전파 값**을 곱해서 전달
*   **Max Gate (최대값 게이트):** **경사 라우터(Gradient Router)**, 순전파 시 **최대값**을 가졌던 입력 브랜치로만 상류 경사를 **라우팅(Route)**하고, 나머지 브랜치에는 0 경사를 전달
*   **다중 출력 노드:** 하나의 노드가 여러 출력에 연결되어 있을 경우, 각 출력으로부터 돌아오는 경사를 **합산(add up)**하여 하류로 전달  

  
### 1-3. Vectorized Gradients and Modularity (벡터화된 경사 및 모듈화)  

*   **Jacobian Matrix (야코비 행렬):** 변수가 벡터일 경우 경사
*   **비효율성:** 야코비 행렬은 매우 클 수 있으므로(예: $4096 \times 4096$ 이상) 전체를 계산하는 것은 비실용적(completely impractical)
*   **대각 행렬 활용:** **요소별 연산(element-wise operations)**(예: ReLU, Max)의 경우, 야코비 행렬은 **대각 행렬(Diagonal Matrix)**이 되기 때문에, 전체 행렬을 계산할 필요 없이 지역적 경사만 계산하여 상류 경사에 곱함  
    -> 요소별 연산은 각 행과 열의 i가 같은 경우만 보기 때문!  
*   **모듈식 API:** 각 노드(게이트)는 `Forward` 함수와 `Backward` 함수를 구현하는 **모듈식 API**를 따름  

  
## 2. Neural Networks (신경망)
: 선형 변환($W \cdot X$)과 **비선형 활성화 함수(Nonlinearity)**를 계층적으로 쌓아 올린 함수들의 집합


### 2-1. Structure and Function (구조와 기능)  

*   **선형 분류기의 확장:** 기존 선형 점수 함수($f = W \cdot X$) 대신, 선형 변환과 **비선형 활성화 함수(Nonlinearity)**를 계층적으로 쌓아 올림
*   **비선형성의 필수성:** 비선형 함수가 없으면 여러 개의 선형 계층을 쌓아도 결국 **하나의 선형 함수로 축소**됨  
    -> 이전에 봤던 도넛 모양 등의 비선형적 분류가 불가능하기 때문  
*   **다층 네트워크의 이점:** 단일 선형 분류기의 **모드 문제(Mode Problem)**(예: 빨간 차만 인식)를 극복
    *   첫 번째 층($W_1$)은 다양한 저수준 템플릿(예: 빨간 차, 노란 차 템플릿)을 학습
    *   다음 층($W_2$)은 이 템플릿들의 점수($H$)를 **가중 합산(Weighted Sum)**하여 '차'와 같은 더 일반적이거나 복잡한 상위 개념을 유연하게 표현 가능!
*   **용어:** 두 개의 선형 계층을 쌓은 네트워크를 **2계층 신경망(Two-layer NN)** 또는 **하나의 은닉층(One Hidden Layer) 신경망**이라고도 부름  

  
### 2-2. Biological Inspiration (생물학적 영감)  

*   **느슨한 비유:** 신경망 노드는 생물학적 뉴런에 **매우 느슨하게(very loose)** 비유됨
    *   **입력($X$)**: 수상돌기(Dendrites)로 들어오는 신호
    *   **선형 합산**: 세포체(Cell Body)에서 신호를 통합(Integrates)함
    *   **활성화 함수**: 뉴런의 **발화율(Firing Rate) 또는 스파이크 비율**에 대한 느슨한 비유로 해석될 수 있음
*   **ReLU의 특징:** **ReLU($\max(0, x)$)**는 입력이 음수일 때 0이고 양수일 때 선형 형태를 가지며, 실제 뉴런의 행동과 **가장 유사한(most similar)** 비선형성 중 하나
*   **주의 사항:** 생물학적 뉴런은 인공 노드보다 **훨씬 더 복잡**함  
    -> 수상돌기는 복잡한 비선형 연산을 수행하고, 시냅스($W$)는 단순한 가중치가 아니라 복잡한 동적 시스템인 경우가 많음

