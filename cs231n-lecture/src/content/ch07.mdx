[CS231N | Spring 2025 | Lecture 7: Recurrent Neural Networks]
https://youtu.be/kG2lAPBF7zA?si=GQgj_wbwOarlgvDI

# 7. Recurrent Neural Networks 

## 1. Sequence Modeling  
  : 기존 Vanilla Neural Network는 고정된 크기의 입력(Fixed-sized input)과 고정된 크기의 출력(Fixed-sized output)만 처리 가능. 하지만 현실의 많은 문제는 가변 길이의 시퀀스(Sequence) 처리가 필요함.

### 1-1. 다양한 입력/출력 형태

1.  **One to One:** 기존의 Vanilla Neural Network (예: 고정 크기 이미지 분류).
2.  **One to Many:** 고정 입력 -> 가변 출력 (예: **Image Captioning** - 이미지 한 장을 입력받아 문장(단어들의 시퀀스)을 출력).
3.  **Many to One:** 가변 입력 -> 고정 출력 (예: **Video Classification** - 비디오 프레임 시퀀스를 입력받아 행동 분류, **Sentiment Analysis** - 문장을 입력받아 감정 분류).
4.  **Many to Many:** 가변 입력 -> 가변 출력.
    *   입력과 출력이 동기화되지 않는 경우 (예: **Machine Translation** - 영어 문장을 다 듣고 한국어 번역 출력).
    *   입력과 출력이 동기화되는 경우 (예: **Video Captioning on frame level** - 매 프레임마다 분류).

## 2. Recurrent Neural Networks (RNN)

### 2-1. 기본 구조 및 원리
  : 시퀀스를 처리하기 위해 **'내부 상태(Hidden State, $h$)'**를 가지는 신경망.

*   **핵심 아이디어:** 매 타임 스텝($t$)마다 새로운 입력($x_t$)과 이전 타임 스텝의 hidden state($h_{t-1}$)를 받아서 현재의 hidden state($h_t$)를 갱신함.
*   이 과정이 반복되므로(Recurrent), 모든 타임 스텝에서 **동일한 가중치 파라미터($W$)를 공유**함.

#### 2-1-1. 수식 표현

*   **Hidden State Update:**
    $h_t = f_W(h_{t-1}, x_t)$
    
    *   $h_t$: 새로운 hidden state
    *   $h_{t-1}$: 이전 hidden state
    *   $x_t$: 현재 시점의 입력 벡터
    *   $f_W$: 파라미터 $W$를 가진 함수

*   **Vanilla RNN (Elman RNN):**
    $h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)$
    $y_t = W_{hy}h_t$

    *   활성화 함수로 주로 **tanh**를 사용.
    *   $W_{hh}$: Hidden state 간의 변환 가중치.
    *   $W_{xh}$: 입력($x$)을 hidden state로 변환하는 가중치.
    *   $W_{hy}$: Hidden state를 출력($y$)으로 변환하는 가중치.

### 2-2. Computational Graph & Backpropagation

#### 2-2-1. Backpropagation Through Time (BPTT)
  : RNN을 시간 순서대로 펼쳐(Unroll) 놓은 뒤, 전체 시퀀스를 통과하며 Loss를 계산하고 역전파를 수행하는 방식.

*   **문제점:** 시퀀스가 매우 길 경우(예: 책 한 권), 전체 시퀀스에 대한 그래디언트를 계산하려면 메모리가 부족하고 계산 비용이 너무 큼.

#### 2-2-2. Truncated BPTT
  : 긴 시퀀스를 작은 덩어리(Chunk)로 잘라서 학습하는 실용적인 방법.

1.  일정 길이(예: 100 step)만큼 순전파(Forward)를 진행하여 Loss 계산.
2.  해당 구간에 대해서만 역전파(Backward) 수행하여 가중치 업데이트.
3.  Hidden state 값은 다음 덩어리로 전달하되, 그래디언트의 흐름은 끊음(Detach).

## 3. RNN Applications

### 3-1. Character-level Language Model
  : 문자(Character) 단위로 다음 글자를 예측하도록 학습하는 모델.

*   예시) 'hello' 학습 시
    *   입력: 'h' -> 출력 예측: 'e' (정답)
    *   입력: 'e' -> 출력 예측: 'l' (정답)
    *   ...
*   **Test Time:** 모델이 예측한 확률분포에서 문자를 하나 샘플링하고, 그 문자를 다음 타임 스텝의 입력으로 다시 넣어주는 방식으로 텍스트 생성.
*   **결과:** 셰익스피어 희곡, 리눅스 커널 코드(C언어) 등을 학습시키면 구조적으로 그럴싸한 텍스트를 생성해냄.

### 3-2. Image Captioning
  : CNN과 RNN을 결합한 모델.

*   **Encoder (CNN):** 이미지를 입력받아 특징 벡터(Vector)를 추출 (주로 FC layer 직전의 feature map 사용).
*   **Decoder (RNN):** CNN이 추출한 벡터를 초기 hidden state($h_0$) 혹은 첫 입력으로 사용하여 문장 생성.
*   `<START>` 토큰으로 시작해서 `<END>` 토큰이 나올 때까지 단어 생성.

### 3-3. Visual Question Answering (VQA)
  : 이미지와 질문(텍스트)을 입력받아 정답을 출력하는 태스크. RNN으로 질문을 인코딩하고 CNN으로 이미지를 인코딩하여 결합.

## 4. Interpretability & Issues

### 4-1. RNN의 해석 가능성 (Interpretability)
  : 학습된 RNN의 Hidden state 중 특정 차원(neuron)이 무엇을 학습했는지 시각화해보면 흥미로운 결과가 나옴.

*   **Quote Detection Cell:** 따옴표(" ") 안의 문장인지를 추적하는 셀.
*   **Line Length Tracking Cell:** 줄 바꿈이 일어날 시점을 예측하기 위해 현재 줄의 길이를 추적하는 셀.
*   **Code Depth Cell:** 코드의 들여쓰기(if문, loop 등) 깊이를 추적하는 셀.

-> 별도의 감독(Supervision) 없이도 모델이 데이터의 구조를 파악하기 위해 스스로 이런 기능을 학습함!

### 4-2. Vanishing & Exploding Gradients
  : Vanilla RNN은 역전파 시 $W_{hh}$ 행렬이 계속해서 곱해지는 구조.

#### 4-2-1. Exploding Gradients (기울기 폭발)

*   원인: $W_{hh}$의 가장 큰 특이값(Singular Value)이 1보다 클 때 발생.
*   **해결책:** **Gradient Clipping**. 그래디언트의 L2 Norm이 임계값(Threshold)을 넘으면 잘라냄(Scale down).

```jsx
grad_norm = np.sum(grad * grad)
if grad_norm > threshold:
    grad *= (threshold / grad_norm)
```

#### 4-2-2. Vanishing Gradients (기울기 소실)

*   원인:
    1.  tanh 함수의 미분값이 항상 1보다 작음 (양 끝에서는 0에 수렴).
    2.  $W_{hh}$의 특이값이 1보다 작으면 계속 곱해지면서 0으로 수렴.
*   결과: 먼 과거의 정보(Long-term dependencies)가 현재까지 전달되지 못하고 사라짐.
*   **해결책:** 아키텍처를 변경해야 함 -> **LSTM, GRU** 등 등장.

## 5. LSTM (Long Short Term Memory)
  : Vanilla RNN의 기울기 소실 문제를 해결하기 위해 고안된 구조 (Hochreiter & Schmidhuber, 1997).

### 5-1. 핵심 구조: Cell State ($c_t$)

*   RNN의 hidden state($h_t$) 외에 **Cell state($c_t$)**라는 별도의 경로를 둠.
*   정보가 큰 방해 없이 흘러갈 수 있는 "고속도로(Highway)" 역할을 하여 기울기 소실을 막음.

### 5-2. 4개의 게이트 (Gates)
  : 시그모이드($\sigma$) 함수를 통해 0~1 사이의 값을 출력하여 정보의 흐름을 제어.

1.  **$i$ (Input gate):** Cell state에 얼마나 새로운 정보를 쓸지 결정.
2.  **$f$ (Forget gate):** 과거의 Cell state 정보를 얼마나 잊을지 결정.
3.  **$o$ (Output gate):** 현재 Cell state를 바탕으로 Hidden state($h_t$)를 얼마나 출력할지 결정.
4.  **$g$ (Gate gate):** Cell에 더해질 새로운 정보의 후보군 (tanh 사용).

```jsx
# LSTM 수식 개요
i = sigmoid(Wx_i * x + Wh_i * h)
f = sigmoid(Wx_f * x + Wh_f * h)
o = sigmoid(Wx_o * x + Wh_o * h)
g = tanh(Wx_g * x + Wh_g * h)

c_t = f * c_{t-1} + i * g  # 핵심: 곱하기가 아닌 더하기(+) 연산 포함!
h_t = o * tanh(c_t)
```

### 5-3. Gradient Flow in LSTM

*   **Additive Interaction:** Cell state 업데이트 식인 $c_t = f \odot c_{t-1} + i \odot g$ 에서 `+` 연산이 존재.
*   역전파 시 `+` 연산은 그래디언트를 그대로 전달(Distribute)하므로, 매번 행렬 곱을 하던 Vanilla RNN에 비해 그래디언트가 훨씬 잘 보존됨.
*   마치 ResNet의 Skip Connection과 유사한 원리로 **Long-term dependency**를 잘 학습함.