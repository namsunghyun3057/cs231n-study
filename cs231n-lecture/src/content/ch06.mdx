[CS231N | Spring 2025 | Lecture 6: CNN Architectures]
https://youtu.be/aVJy4O5TOk8?si=uZWoCqdvBdVt8EaY

# 6. CNN Architectures  

## 1. Layers in CNNs  

### 1-1. Normalization Layers(정규화 계층)  
  : 입력 데이터의 평균과 표준편차 통계를 계산하여 정규화한 뒤, 학습 가능한 파라미터로 스케일(scale)과 시프트(shift)를 적용하는 과정.  

![image.png](/images/ch06/ch06_01.png)  

**차원 표기 설명 ($N, C, H, W$)**
: 강의에서 정규화 방식을 설명할 때 데이터의 형태(Shape)를 이해하는 것이 필수적! CNN에서 이미지가 처리될 때 데이터는 주로 4차원 텐서(Tensor)로 표현.  

*   **$N$ (Batch Size, 배치 크기):** 한 번의 학습 단계(iteration)에서 모델에 입력되는 이미지 샘플의 개수.  
*   **$C$ (Channel/Depth, 채널 또는 깊이):** 데이터의 깊이(depth). 입력 이미지의 경우 RGB 3채널이지만, 네트워크를 통과하면서 필터 개수에 따라 채널 수가 변함(예: 64, 128 등).  
*   **$H, W$ (Height, Width, 높이와 너비):** 이미지(또는 특징 맵)의 공간적(Spatial) 크기.  
-> 즉, 정규화 계층은 이 **$(N, C, H, W)$** 텐서에서 **어떤 축(dimension)을 기준으로** 평균과 표준편차를 계산할 것인지에 따라 종류가 나뉨!

* 위에서 설명했듯 모든 정규화 레이어가 학습 가능한 파라미터를 적용한다는 점은 같지만, **통계를 계산하는 부분 집합(subset)**이 다르다!
   - Batch Norm: $N$축을 따라 계산. 배치 내의 '모든 샘플($N$)과 공간($H, W$)'의 묶음에 대해 '각 채널별($C$)'로 평균과 표준편차 계산.  
   - Layer Norm: $N$축을 따라가지 않고 '각 샘플별($N$)'로 채널($C$)과 공간($H, W$) 차원에 대해 평균과 표준편차 계산.  
                  -> Transformer Model  
   - Instance Norm: 가장 세분화된 방식. '각 샘플($N$)'의 '각 채널($C$)'마다 개별 공간($H, W$)으로 통계 계산.  
   - Group Norm: Layer Norm과 Instance Norm의 중간 단계. '하나의 샘플($N$)' 안에서, 채널을 소그룹(Subset of $C$)으로 나눈 뒤 공간 차원($H, W$)을 합쳐 통계 계산.  

### 1-2. Dropout(드롭아웃)
  : 과적합을 방지하고 일반화 성능을 높이기 위해 CNN 모델 아키텍처에 적용하는 정규화 기법.  

![image.png](/images/ch06/ch06_02.png)  

#### 1-2-1. 작동 원리
*   **무작위 비활성화:** 학습 과정(Training)에서 각 레이어의 순전파(Forward pass)를 진행할 때, 출력값(Activations)의 일부를 무작위로 0으로 설정.  
*   **하이퍼파라미터 ($p$):** 값을 0으로 만들 확률(또는 유지할 확률)은 고정된 하이퍼파라미터(보통 0.5나 0.25가 가장 흔하게 사용됨).  
*   **효과:** 이렇게 하면 모델은 학습 데이터의 모든 정보를 온전히 사용할 수 없게 되므로, 학습 데이터에 대한 성능(Training Error)은 오히려 나빠짐. 하지만 결과적으로 테스트 데이터에 대한 성능(Generalization)은 좋아지게 됨.  
    -> 결국 학습 과정에서는 성능이 낮지만, 이 방법을 통해 학습한 모델이 실전에서는 오히려 성능이 높다고 함!  

#### 1-2-2. 사용 이유
  : **'중복된 표현(Redundant Representations)'**을 학습하도록 강제하기 때문  

*   예시) Cat 분류 문제
![image.png](/images/ch06/ch06_03.png)  
 - 만약 Dropout이 없다면, 모델은 "털이 있고(furry) + 귀가 있으면(ear) = 고양이"라는 특정 특징들의 조합에만 과도하게 의존할 수 있음.
 - 하지만 Dropout으로 인해 '귀'라는 특징이 무작위로 사라질 수 있음.  
    -> 모델은 '귀'가 없는 상황에서도 고양이를 맞추기 위해 '꼬리(tail)'나 '발톱(claws)' 같은 다른 특징들도 함께 보거나, 특징들 간의 더 넓은 관계를 학습해야함!  
 - 결국, 특정 특징 하나가 없어도 정답을 맞출 수 있도록 네트워크가 더 견고(Robust)해지며, 과적합을 방지하게 됨!!!  

#### 1-2-3. 실제 예측 단계에서의 처리
  : 학습 때는 무작위성을 부여하지만, **실제 예측 단계에서는 드롭아웃을 적용하지 않음**. 모든 뉴런을 사용하여 가장 좋은 예측을 해야 하기 때문. 이때 주의할 점은 **스케일링(Scaling)**.  

```jsx
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
```

*   **문제점:** 학습 때는 50%의 뉴런만 활성화되었다면, 실제 예측에서는 100%의 뉴런이 활성화됨. 따라서 다음 레이어로 들어가는 입력값의 총량(Magnitude)이 학습 때보다 2배 커질 수 있음.  
*   **해결책:** 학습 때와 실제 예측의 입력값 크기(Magnitude)를 맞춰주기 위해, 실제 예측 단계에서 모든 출력값에 **드롭아웃 확률 $p$를 곱해줌**.  
    *   예를 들어, 학습 때 50%를 껐다면($p=0.5$), 실제 예측 단계의 출력값에 0.5를 곱해서 크기를 줄여줌. 이를 통해 모델의 분산(Variance)과 속성을 유지.  

#### 1-2-4. 역전파 과정
  : Dropout이 적용된 상태에서 역전파 작동 방식

*   **기울기(Gradient) 차단:** 순전파 때 0으로 설정된(드롭된) 값은 역전파 때도 기울기가 0. 마치 ReLU 함수에서 음수 입력값의 기울기가 0이 되는 것과 유사.  
*   **가중치 업데이트:** 드롭된 활성값과 연결된 가중치(Weights)들은 해당 단계(Step)에서 업데이트되지 않음.  

## 2. Activation Functions  

### 2-1. Sigmoid
  : 과거에는 자주 사용되었으나, 현재는 **기울기 소실(Vanishing Gradient)** 문제 때문에 거의 사용되지 않음.  

*   **문제점:** 역전파(Backpropagation) 과정에서 출력층에서 입력층으로 갈수록 기울기(Gradient)가 점점 작아져서 소멸하는 현상이 발생.  
*   **원인:** Sigmoid 그래프를 보면 입력값이 아주 크거나(양수) 아주 작으면(음수) 기울기가 거의 0에 수렴. 기울기가 0이 아닌 구간은 중앙의 아주 좁은 영역뿐.  
*   **결과:** 입력값이 조금만 커지거나 작아져도 기울기가 죽어버려(kill), 깊은 네트워크의 앞쪽 레이어들이 학습되지 않는 치명적인 단점 발생.  

### 2-2. ReLU (Rectified Linear Unit)
  : Sigmoid의 문제를 해결하기 위해 등장하여 가장 대중적으로 사용되는 함수.
  $f(x) = \max(0, x)$

*   **장점 1 (기울기 보존):** 양수 입력($x > 0$)에 대해서는 기울기가 항상 1. 따라서 레이어가 깊어져도 양수 영역에서는 기울기 소실 문제가 발생하지 않음.  
*   **장점 2 (연산 효율성):** 지수 함수 연산이 필요한 Sigmoid와 달리, 단순히 0과의 최댓값만 취하면 되므로 계산 비용이 훨씬 저렴함.  
*   **단점:** 음수 입력($x < 0$)에 대해서는 기울기가 0. 이는 Sigmoid보다는 낫지만(Sigmoid는 양쪽 다 0으로 수렴하므로), 여전히 입력 데이터의 절반가량에 대해 기울기가 사라진다는 한계 존재.  

### 2-3. 최신 활성화 함수 (GeLU, SiLU/CeLU)
  : 최근, 특히 트랜스포머(Transformer) 모델 등에서 ReLU의 단점을 보완하기 위해 사용되는 함수들.  

*   **GeLU (Gaussian Error Linear Unit):** 가우시안 정규 분포의 누적 분포 함수(CDF)를 활용한 것.  
    *   **특징:** ReLU와 비슷하게 생겼지만, 0 부근에서 꺾이는 부분이 **부드러운 곡선(Smooth)** 형태.  
    *   입력이 아주 작으면 0에 수렴하고 아주 크면 $x$에 수렴하지만, 중간 영역에서 부드럽게 연결되며 음수 영역에서도 0이 아닌 기울기!  
*   **SiLU / CeLU:** 강의에서는 $x \cdot \text{sigmoid}(x)$ 형태(일반적으로 Swish 혹은 SiLU라 불림)를 언급하며, 이 역시 GeLU와 매우 비슷한 모양을 가진다고 함.  
*   **핵심:** ReLU의 급격한 꺾임(Sharp jump)을 부드럽게 만들고, 0 근처의 음수 영역에서도 미세한 기울기를 허용하여 학습을 도움.  

### 2-4. 사용 위치
  : Activation Function은 주로 선형 연산 직후에 배치.
*   예시) 컨볼루션 계층(Convolution Layer)이나 완전 연결 계층(Fully Connected Layer)을 통과한 뒤 바로 적용.  

## 3. CNN Architectures

### 3-1. VGGNet  
  : 2010년대에 등장한 모델로, 이전 모델인 AlexNet보다 더 깊은 층을 쌓으면서도 **구조를 단순화**하고 **3x3 컨볼루션**을 적극적으로 활용한 것이 특징.

#### 3-1-1. 구조적 특징
*   **단순하고 규칙적인 블록:** VGGNet은 3x3 크기의 컨볼루션 레이어(Stride 1, Padding 1)를 여러 번 반복해서 쌓고, 그 뒤에 2x2 Max Pooling(Stride 2)을 배치하는 패턴을 사용.
*   **완전 연결 계층 (Fully Connected Layers):** 컨볼루션 층을 모두 통과한 후, 4096차원의 FC 레이어 2개와 1000차원의 분류 레이어(ImageNet 클래스 개수)를 배치.

#### 3-1-2. 핵심 설계 철학  
  : VGGNet의 가장 큰 기여는 큰 필터(예: 7x7) 하나를 쓰는 대신 **작은 3x3 필터를 여러 개 쌓는 것이 더 유리하다**는 것을 입증한 점. 강의에서는 이를 세 가지 관점에서 설명.

![image.png](/images/ch06/ch06_04.png)  

  1.  **유효 수용 영역(Effective Receptive Field) 유지:**
    *   3x3 컨볼루션 레이어를 3번 쌓으면, 입력 이미지 상에서 **7x7 영역**의 정보. 즉, 7x7 필터 1개와 동일한 수용 영역을 가질 수 있음!  
    *   1번째 레이어: 3x3 영역 참조
    *   2번째 레이어: 1번째 레이어의 3x3을 참조하므로, 원본의 5x5 영역 참조
    *   3번째 레이어: 원본의 7x7 영역 참조

  2.  **비선형성(Nonlinearity) 증가:**
    *   7x7 필터 1개는 활성화 함수(ReLU)를 한 번만 사용.  
    *   반면, 3x3 필터를 3번 쌓으면 레이어 사이마다 ReLU를 넣을 수 있어 총 **세 번의 비선형성**을 확보. 이를 통해 모델은 더 복잡하고 구별력 있는 특징(discriminative features)을 학습.  

  3.  **파라미터 수 감소 (효율성):**
    *   입출력 채널이 $C$일 때, 7x7 필터 1개의 파라미터 수는 $49C^2$  
    *   3x3 필터 3개의 파라미터 수는 $3 \times (9C^2) = 27C^2$  
    *   결과적으로 VGGNet 방식은 더 적은 파라미터로 동일한 영역을 보면서 더 깊은 학습을 가능케 함.  

### 3-2. ResNet  
  : "네트워크가 깊어질수록 성능이 좋아야 하는데, 왜 실제로는 더 나빠지는가?"라는 의문에서 출발, 초심층 네트워크 학습을 가능하게 만든 아키텍처.

![image.png](/images/ch06/ch06_05.png)  

#### 3-2-1. 문제 제기: 최적화 난이도 (Optimization Issue)
*   연구진은 20층 모델보다 56층 모델이 테스트 에러뿐만 아니라 **학습 에러(Training Error)**조차 더 높다는 것을 발견.
*   만약 과적합 문제였다면 학습 에러는 낮아야 함. 따라서 이는 모델이 깊어질수록 **최적화(Optimization)하기가 너무 어려워진다**는 것을 의미!  
*   **이론적 직관:** 깊은 모델은 얕은 모델이 할 수 있는 모든 것을 할 수 있어야 함. 예를 들어, 추가된 레이어들이 입력을 그대로 출력하는 **항등 함수(Identity Mapping)**만 학습한다면, 깊은 모델은 최소한 얕은 모델만큼의 성능은 내야 함.

#### 3-2-2. 해결책: 잔차 학습 (Residual Learning)**
  : 이 문제를 해결하기 위해 ResNet은 **스킵 연결(Skip Connection)** 또는 **잔차 블록(Residual Block)**이라는 개념을 도입.

*   **기존 방식:** 입력 $x$를 받아 출력 $H(x)$를 직접 학습하려고 시도.
*   **ResNet 방식:** 입력 $x$를 출력에 그대로 합함. 즉, 네트워크는 $H(x) = F(x) + x$가 되도록 구성되며, 레이어는 잔차(차이)인 **$F(x)$**만 학습하면 됨.
    *   $F(x)$는 컨볼루션 레이어들을 거친 결과이고, 여기에 원본 $x$를 합해줌(Element-wise addition).
*   **효과:** 만약 추가된 레이어가 아무것도 할 필요가 없다면(Identity Mapping이 최적이라면), 모델은 단순히 $F(x)$의 가중치를 0으로 만들면 됨. 처음부터 완벽한 매핑을 학습하는 것보다, 0(잔차)을 학습하는 것이 훨씬 쉽기 때문에 최적화가 원활.  

```jsx
요약: 
이론적 배경: 깊은 모델은 얕은 모델의 기능을 그대로 흉내 낼 수 있으므로(y=x), 성능이 더 나빠져선 안된다.
문제: 일반적인 네트워크는 "그대로 흉내 내는 것(y=x)"을 학습하는 게 어렵다고 한다.
해결: ResNet은 지름길(+x)을 뚫어주어, "추가로 배울 게 없으면 그냥 가중치를 0으로 만들라(F(x) -> 0)"고 유도한다.
결론: 0을 만드는 것은 매우 쉽기 때문에, ResNet은 층이 아무리 깊어져도 "아무것도 안 하는 층(Identity)"을 통해 기본 성능을 보장할 수 있게 된다.
```

#### 3-2-3. 아키텍처 및 성과**
*   **구성:** VGG처럼 3x3 컨볼루션을 주로 사용하며, 스킵 연결을 위해 텐서의 크기를 일정하게 유지. 주기적으로 필터 수를 2배로 늘리고 공간 크기(Spatial dimension)를 절반으로 줄여 계산량을 조절.  
*   **성과:** 이러한 구조로써 152층(ResNet-152) 같은 매우 깊은 모델도 성공적으로 학습.

## 4. Weight Initialization

### 4-1. 가중치 초기화가 중요한 이유 (The "Goldilocks" Problem)
*   **초기값이 너무 작을 때 (Too Small):**
    *   가중치를 무작위로 생성한 후 `0.01`을 곱해 아주 작은 값으로 초기화하면, 층을 통과할수록 활성값(Activations)의 평균과 표준편차가 점점 작아짐.
    *   결과적으로 깊은 레이어에 도달하면 신호가 0에 수렴하여(vanishing), 학습이 제대로 이루어지지 않음.
*   **초기값이 너무 클 때 (Too Large):**
    *   가중치에 `0.05` 같은 값을 곱해 너무 크게 잡으면, 층을 거칠수록 값이 증폭됨.
    *   특히 ReLU와 같이 양수 값을 제한하지 않는 활성화 함수를 사용할 경우, 큰 가중치가 계속 곱해지면서 마지막 레이어에서는 값이 무한대로 발산하거나 계산 불가능한 상태가 됨.

따라서, 가장 이상적인 상태는 **모든 레이어에서 활성값의 평균과 표준편차가 일정하게 유지되는 것**!  

### 4-2. 해결책: Kaiming Initialization
*   **배경:** ReLU 활성화 함수를 사용하는 네트워크를 위해 고안. ReLU는 입력의 절반(음수)을 0으로 만들기 때문에, 이를 보정해주지 않으면 층이 깊어질수록 분산이 줄어드는 문제가 발생.
*   **공식:** 가중치를 초기화할 때, 무작위 정규분포(Gaussian) 값에 **$\sqrt{2 / \text{입력 차원}}$**을 곱해줌.
    *   여기서 $\sqrt{2}$라는 계수가 ReLU로 인해 절반이 0이 되는 현상을 보상하여, 레이어를 통과해도 표준편차가 일정하게 유지되도록 함.
*   **효과:** 이 공식을 사용하면 별도의 복잡한 유도 과정 없이도 모델의 신호가 사라지거나 폭발하지 않고 안정적으로 학습될 수 있음.  

### 4-3. CNN에서의 적용 방법

![image.png](/images/ch06/ch06_06.png)  

*   **완전 연결 계층:** 입력 뉴런의 수(예: 4096)가 바로 입력 차원.
*   **CNN (Convolutional Layer):** 필터(커널)의 크기 고려.
    *   예를 들어 $3 \times 3$ 크기의 필터를 사용하고 입력 채널이 6개라면, 입력 차원은 $3 \times 3 \times 6$
    *   즉, **(커널 높이 $\times$ 커널 너비 $\times$ 입력 채널 수)**를 입력 차원으로 간주하고 공식을 적용.  

## 5. CNN Training

### 5-1. Data Preprocessing
  : 이미지 데이터를 모델에 입력하기 전에 준비하는 과정.

*   **정규화 (Normalization):** 이미지는 픽셀 단위로 구성되어 있으므로 처리가 비교적 간단. 전체 학습 데이터셋에 대해 각 채널(R, G, B)별로 **평균(Mean)**과 **표준편차(Standard Deviation)**를 계산.
    *   입력 이미지가 들어오면 미리 구해둔 평균을 빼고 표준편차로 나누어 정규화.
*   **실전 팁:** 매번 내 데이터셋의 평균/표준편차를 새로 계산하기보다, ImageNet과 같은 대규모 데이터셋의 통계치를 그대로 가져와서 사용하는 경우가 매우 흔하며 효과적이라고 함!
pca/ w 안쓰는 이유?

### 5-2. Data Augmentation
  : 데이터 증강은 학습 시 이미지에 인위적인 변형을 가해 데이터의 다양성을 확보하고 과적합(Overfitting)을 방지하는 필수적인 기법.

**A. 학습 단계 (Training Time)**
*   **무작위성 주입:** 원본 이미지를 그대로 쓰는 대신, 매번 무작위 변형을 가해 모델이 같은 이미지를 다르게 인식하도록 만듦. 이는 모델이 특정 픽셀값이나 패턴을 단순히 암기하는 것을 방지.
*   **주요 기법:**
    *   **Horizontal Flip (좌우 반전):** 대부분의 객체 인식에 유용하지만, 텍스트 인식 등 방향이 중요한 경우에는 사용하면 안됨.
    *   **Random Resized Crop (무작위 크기 조절 및 자르기):** 가장 널리 쓰이는 방법. 이미지의 짧은 변을 특정 크기($L$)로 리사이징한 뒤, 무작위 위치에서 224x224 같은 고정 크기로 자르기.
    *   **Color Jitter:** 이미지의 밝기(brightness)나 대비(contrast)를 무작위로 조절.

**B. 테스트 단계 (Test Time)**
*   **Test Time Augmentation (TTA):** 테스트 시 성능을 극대화하기 위한 팁.
    *   테스트 이미지를 단순히 하나만 넣는 것이 아니라, 여러 번 증강(예: 여러 위치의 크롭, 좌우 반전 등)하여 모델에 입력.
    *   나온 예측값들의 **평균**을 내어 최종 결과를 도출하면, 보통 1~2% 정도의 성능 향상을 보인다고 함.

### 5-3. Transfer Learning
  : 데이터가 충분하지 않을 때(예: ImageNet처럼 100만 장이 없을 때) 사용. 대규모 데이터셋으로 미리 학습된(Pre-trained) 모델을 가져와 내 문제에 맞게 재사용하는 방법.

**A. 작동 원리**
*   CNN의 앞단(Conv Layers)은 일반적인 특징(모서리, 색상, 질감 등)을 추출하고, 뒷단(FC Layers)으로 갈수록 고차원적인 특징을 추출.
*   미리 학습된 모델의 **특징 추출 능력(Feature Extractor)**은 다른 데이터셋에서도 유효하게 작동.

**B. 적용 전략 (데이터 양에 따른 구분)**
1.  **데이터가 적을 때 (Linear Classifier):**
    *   CNN 전체(Backbone)의 가중치는 고정(Freeze)하고, 마지막 분류 레이어(Linear Layer)만 내 데이터셋의 클래스 개수에 맞춰 교체한 뒤 학습.
2.  **데이터가 어느 정도 많을 때 (Fine-tuning):**
    *   사전 학습된 가중치로 초기화하되, 모델 전체(모든 레이어)를 미세하게 재학습. 처음부터 학습하는 것보다 훨씬 성능이 좋고 수렴도 빠르다고 함.

**C. 주의사항**
*   사전 학습 데이터(ImageNet)와 내 데이터(예: 화성 표면 사진)의 분포가 너무 다르면(Out of Distribution), 전이 학습의 효과가 떨어질 수 있음! 이 경우 바닥부터 학습(Training from scratch)하거나 도메인 적응(Domain Adaptation) 기술이 필요.

### 5-4. Hyperparameter Selection
  : 학습률(Learning Rate)이나 배치 크기 등을 결정하는 실전 팁입니다.

**A. 디버깅 전략 (Overfit on a small sample)**
*   본격적인 학습 전에, **아주 적은 데이터(예: 1~2개 샘플)**만 가지고 모델을 학습시켜 보기.
*   이때 손실(Loss)이 0으로 떨어지며 완벽하게 과적합(Overfitting)되는지 확인. 만약 되지 않는다면 코드에 버그가 있거나 모델 설계가 잘못된 것.

**B. 탐색 전략 (Grid Search vs. Random Search)**
*   **Coarse to Fine:** 처음에는 넓은 범위에서 대략적으로 탐색하고, 좋은 범위가 발견되면 그 구간을 세밀하게 탐색.
*   **Random Search 권장:** 격자 모양으로 모든 조합을 다 해보는 Grid Search보다, 무작위로 값을 뽑는 **Random Search**가 더 효율적.
    *   이유: 어떤 하이퍼파라미터는 성능에 큰 영향을 주고(중요), 어떤 것은 덜 중요. Random Search는 중요한 파라미터의 공간을 더 촘촘하게 탐색 가능.
